{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CIefhEVc8tT",
        "outputId": "ac9eeaf7-9243-4d85-9f4a-4d9799d582e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn nltk pandas numpy imbalanced-learn seaborn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "doEvQvuzd2TR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data package\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNnuBFYJXEF0",
        "outputId": "a0ade0e9-451c-4aa0-febb-830958426bab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path ke file di Google Drive\n",
        "file_path = '/content/drive/My Drive/dicoding/ulasan.csv'\n",
        "\n",
        "# Membaca file CSV\n",
        "data = pd.read_csv(file_path)\n",
        "print(data.info())\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gkgp7U-neGqy",
        "outputId": "20079591-41ad-421e-a1b4-9a334859fedb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6018 entries, 0 to 6017\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   at        6018 non-null   object\n",
            " 1   userName  6018 non-null   object\n",
            " 2   score     6018 non-null   int64 \n",
            " 3   content   6018 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 188.2+ KB\n",
            "None\n",
            "                    at                userName  score  \\\n",
            "0  2024-11-22 03:25:50         Badrin Tholchah      5   \n",
            "1  2024-11-21 17:58:03  Cevin Agista Pramudito      5   \n",
            "2  2024-11-21 15:30:17            Nur Kholikoh      1   \n",
            "3  2024-11-21 01:00:19           Xavier Borkan      5   \n",
            "4  2024-11-20 05:45:10           Andri Chandra      5   \n",
            "\n",
            "                                             content  \n",
            "0                             good taste and service  \n",
            "1    Aplikasinya bagus dan sangat membantu pembelian  \n",
            "2  Hati-hati aplikasi ini melakukan phising. Memi...  \n",
            "3                                         ok terbaik  \n",
            "4                                               good  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRIef-TmfCC_",
        "outputId": "0a39dea3-e3d5-48cb-9d5a-6f9baf726f4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SLANG_DICT = {\n",
        "    # Negative sentiments\n",
        "    'gak': 'tidak', 'ga': 'tidak', 'nggak': 'tidak', 'ngga': 'tidak',\n",
        "    'gk': 'tidak', 'g': 'tidak', 'kgk': 'tidak',\n",
        "    'buruk': 'buruk', 'jelek': 'buruk', 'parah': 'buruk',\n",
        "    'ancur': 'buruk', 'hancur': 'buruk', 'zonk': 'buruk',\n",
        "\n",
        "    # Positive sentiments\n",
        "    'mantap': 'bagus', 'mantul': 'bagus', 'manteb': 'bagus',\n",
        "    'oke': 'baik', 'ok': 'baik', 'okee': 'baik', 'okey': 'baik',\n",
        "    'sip': 'baik', 'asiap': 'baik', 'asik': 'baik',\n",
        "    'enak': 'bagus', 'enk': 'bagus',\n",
        "    'bgt': 'sangat', 'banget': 'sangat', 'bgets': 'sangat',\n",
        "    'recommended': 'direkomendasikan', 'rekomen': 'direkomendasikan',\n",
        "    'worth': 'sepadan', 'worthit': 'sepadan',\n",
        "\n",
        "    # Service related\n",
        "    'fast': 'cepat', 'quick': 'cepat', 'lambat': 'lama',\n",
        "    'respon': 'respons', 'response': 'respons',\n",
        "    'ramah': 'baik', 'friendly': 'baik',\n",
        "\n",
        "    # Common expressions\n",
        "    'thx': 'terima kasih', 'thanks': 'terima kasih', 'tks': 'terima kasih',\n",
        "    'tq': 'terima kasih', 'ty': 'terima kasih'\n",
        "}"
      ],
      "metadata": {
        "id": "ZdCLCdA_Wqms"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Enhanced text preprocessing\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove URLs and HTML tags\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+|<.*?>', '', text)\n",
        "\n",
        "    # Handle repeated characters (e.g., 'baguuuusss' -> 'bagus')\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "    # Preserve important punctuation patterns\n",
        "    text = re.sub(r'[!]{2,}', ' sangat ', text)  # Multiple exclamation marks indicate intensity\n",
        "    text = re.sub(r'[?]{2,}', ' bingung ', text)  # Multiple question marks might indicate confusion\n",
        "\n",
        "    # Handle emoticons\n",
        "    text = re.sub(r'[:;][\\-]?[)dp]', ' senang ', text)  # Happy emoticons\n",
        "    text = re.sub(r'[:;][\\-]?[\\(]', ' sedih ', text)    # Sad emoticons\n",
        "\n",
        "    # Tokenize\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Apply slang conversion\n",
        "    words = [SLANG_DICT.get(word, word) for word in words]\n",
        "\n",
        "    # Handle negation\n",
        "    negation_words = {'tidak', 'bukan', 'kurang', 'belum'}\n",
        "    processed_words = []\n",
        "    negate = False\n",
        "\n",
        "    for word in words:\n",
        "        if word in negation_words:\n",
        "            negate = True\n",
        "            continue\n",
        "        if negate:\n",
        "            word = f\"tidak_{word}\"\n",
        "            negate = False\n",
        "        processed_words.append(word)\n",
        "\n",
        "    # Remove stopwords but preserve important ones\n",
        "    stop_words = set(stopwords.words('indonesian'))\n",
        "    custom_stopwords = {'yg', 'dgn', 'ny', 'tk', 'klo', 'gk', 'krn', 'lg', 'sih', 'nya', 'deh', 'sih'}\n",
        "    stop_words.update(custom_stopwords)\n",
        "    important_words = {'tidak', 'sangat', 'kurang', 'bukan', 'belum', 'baik', 'buruk', 'bagus', 'cepat', 'lama'}\n",
        "\n",
        "    words = [word for word in processed_words if (word not in stop_words or word in important_words)]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "RhUzObUtfG1M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced sentiment labeling with confidence threshold\n",
        "def label_sentiment(score):\n",
        "    if score >= 4:\n",
        "        return 'positive'\n",
        "    elif score <= 2:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'"
      ],
      "metadata": {
        "id": "KSOqY0HafPIH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "def train_sentiment_model(data):\n",
        "    \"\"\"Train sentiment model with improved handling of class imbalance\"\"\"\n",
        "    print(\"Preprocessing text...\")\n",
        "    data['cleaned_content'] = data['content'].apply(preprocess_text)\n",
        "    data['sentiment'] = data['score'].apply(label_sentiment)\n",
        "\n",
        "    # Prepare data\n",
        "    X = data['cleaned_content']\n",
        "    y = data['sentiment']\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Enhanced TF-IDF vectorization\n",
        "    print(\"Vectorizing text...\")\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 3),  # Include trigrams\n",
        "        min_df=2,\n",
        "        max_df=0.95,\n",
        "        strip_accents='unicode',\n",
        "        sublinear_tf=True\n",
        "    )\n",
        "\n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "    # Apply SMOTE for better balance\n",
        "    print(\"Balancing dataset...\")\n",
        "    smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_vectorized, y_train)\n",
        "\n",
        "    # Use RandomForest with optimized parameters\n",
        "    print(\"Training model...\")\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        class_weight='balanced',\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_balanced, y_train_balanced)\n",
        "    # Evaluate\n",
        "    print(\"\\nModel Evaluation:\")\n",
        "    y_pred = model.predict(X_test_vectorized)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model, vectorizer"
      ],
      "metadata": {
        "id": "ZPCN8MpZfRF9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "def main():\n",
        "    # Read data\n",
        "    # Train model\n",
        "    model, vectorizer = train_sentiment_model(data)\n",
        "\n",
        "    # Print sample predictions\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    sample_reviews = data['content'].head()\n",
        "    for review in sample_reviews:\n",
        "        cleaned = preprocess_text(review)\n",
        "        vectorized = vectorizer.transform([cleaned])\n",
        "        prediction = model.predict(vectorized)[0]\n",
        "        print(f\"\\nReview: {review}\")\n",
        "        print(f\"Sentiment: {prediction}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMhiwFpqU5yY",
        "outputId": "3c35ca69-eb7c-410f-9209-12c8411b2020"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing text...\n",
            "Vectorizing text...\n",
            "Balancing dataset...\n",
            "Training model...\n",
            "\n",
            "Model Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.65      0.62      0.64       192\n",
            "     neutral       0.10      0.10      0.10        41\n",
            "    positive       0.92      0.93      0.93       971\n",
            "\n",
            "    accuracy                           0.85      1204\n",
            "   macro avg       0.56      0.55      0.55      1204\n",
            "weighted avg       0.85      0.85      0.85      1204\n",
            "\n",
            "\n",
            "Sample Predictions:\n",
            "\n",
            "Review: good taste and service\n",
            "Sentiment: positive\n",
            "\n",
            "Review: Aplikasinya bagus dan sangat membantu pembelian\n",
            "Sentiment: positive\n",
            "\n",
            "Review: Hati-hati aplikasi ini melakukan phising. Meminta otp dan login ke akun telegram saya lalu menyebarkan pesan berisi link phising juga. Hati2 bila data anda disalahgunakan!!\n",
            "Sentiment: negative\n",
            "\n",
            "Review: ok terbaik\n",
            "Sentiment: positive\n",
            "\n",
            "Review: good\n",
            "Sentiment: positive\n"
          ]
        }
      ]
    }
  ]
}